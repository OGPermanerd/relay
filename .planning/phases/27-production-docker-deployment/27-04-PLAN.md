---
phase: 27-production-docker-deployment
plan: 04
type: execute
wave: 2
depends_on: ["27-01", "27-02"]
files_modified:
  - docker/docker-compose.prod.yml
  - docker/.env.example
autonomous: true

must_haves:
  truths:
    - "docker compose -f docker/docker-compose.prod.yml config validates without errors"
    - "Compose defines 3 services: caddy, web, postgres with correct dependency chain"
    - "PostgreSQL has NO host port mapping (internal network only)"
    - "Caddy binds to public IP 178.156.181.178 only (avoids Tailscale conflict on :443)"
    - "All 3 services have health checks"
    - "Named volumes exist for postgres_data, caddy_data, caddy_config"
    - ".env.example documents every required environment variable"
  artifacts:
    - path: "docker/docker-compose.prod.yml"
      provides: "3-service orchestration with health checks"
      contains: "pgvector/pgvector:pg17"
    - path: "docker/.env.example"
      provides: "Environment variable template"
      contains: "DB_PASSWORD"
  key_links:
    - from: "docker/docker-compose.prod.yml"
      to: "docker/Dockerfile"
      via: "web service build context"
      pattern: "dockerfile.*Dockerfile"
    - from: "docker/docker-compose.prod.yml"
      to: "docker/Dockerfile.caddy"
      via: "caddy service build"
      pattern: "Dockerfile.caddy"
    - from: "docker/docker-compose.prod.yml"
      to: "docker/Caddyfile"
      via: "caddy volume mount"
      pattern: "Caddyfile.*ro"
---

<objective>
Create the Docker Compose production file and environment variable template that ties everything together.

Purpose: docker-compose.prod.yml orchestrates all three services (Caddy, Next.js, PostgreSQL) with health checks, dependency ordering, and network isolation. The .env.example serves as the configuration template for production deployment. This plan depends on Plans 01 and 02 because it references the Dockerfile, Dockerfile.caddy, Caddyfile, and /api/health endpoint they create.

Output: docker/docker-compose.prod.yml, docker/.env.example
</objective>

<execution_context>
@/home/dev/.claude/get-shit-done/workflows/execute-plan.md
@/home/dev/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create docker-compose.prod.yml and .env.example</name>
  <files>
    docker/docker-compose.prod.yml
    docker/.env.example
  </files>
  <action>
1. Create `docker/docker-compose.prod.yml`:

```yaml
services:
  caddy:
    build:
      context: .
      dockerfile: Dockerfile.caddy
    container_name: everyskill-caddy
    ports:
      - "178.156.181.178:80:80"
      - "178.156.181.178:443:443"
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy_data:/data
      - caddy_config:/config
    environment:
      HETZNER_DNS_API_TOKEN: ${HETZNER_DNS_API_TOKEN}
    depends_on:
      web:
        condition: service_healthy
    networks:
      - public
      - internal
    restart: unless-stopped

  web:
    build:
      context: ../
      dockerfile: docker/Dockerfile
    container_name: everyskill-web
    environment:
      DATABASE_URL: postgresql://everyskill:${DB_PASSWORD}@postgres:5432/everyskill
      NEXTAUTH_URL: https://everyskill.ai
      AUTH_SECRET: ${AUTH_SECRET}
      AUTH_GOOGLE_ID: ${AUTH_GOOGLE_ID}
      AUTH_GOOGLE_SECRET: ${AUTH_GOOGLE_SECRET}
      NEXT_PUBLIC_ROOT_DOMAIN: everyskill.ai
      NEXT_PUBLIC_APP_URL: https://everyskill.ai
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      VOYAGE_API_KEY: ${VOYAGE_API_KEY}
      R2_ENDPOINT: ${R2_ENDPOINT}
      R2_ACCESS_KEY_ID: ${R2_ACCESS_KEY_ID}
      R2_SECRET_ACCESS_KEY: ${R2_SECRET_ACCESS_KEY}
      R2_BUCKET_NAME: ${R2_BUCKET_NAME}
      ADMIN_EMAILS: ${ADMIN_EMAILS}
      NODE_ENV: production
    expose:
      - "2000"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:2000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    networks:
      - internal
    restart: unless-stopped

  postgres:
    image: pgvector/pgvector:pg17
    container_name: everyskill-postgres
    environment:
      POSTGRES_USER: everyskill
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: everyskill
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U everyskill"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - internal
    restart: unless-stopped
    # NO ports: section -- database is internal only (SOC2-07)

volumes:
  postgres_data:
  caddy_data:
  caddy_config:

networks:
  internal:
    internal: true
  public:
```

Key details:
- Caddy binds to `178.156.181.178:80` and `178.156.181.178:443` -- NOT 0.0.0.0. This avoids conflicting with Tailscale which is also on port 443 on the VPS.
- PostgreSQL has NO `ports:` section. It is only accessible within the `internal` Docker network (SOC2-07).
- web service uses `expose: ["2000"]` (internal only, no host mapping).
- Health check cascade: postgres (pg_isready) -> web (/api/health) -> caddy (depends_on web healthy).
- web service has `start_period: 30s` to give Next.js time to start before health checks begin.
- VOYAGE_API_KEY included (used by embedding features).
- All secrets come from env vars, never baked into images.

2. Create `docker/.env.example`:

```bash
# EverySkill Production Environment Variables
# Copy to .env and fill in all values before running docker compose

# === Database ===
DB_PASSWORD=                          # Strong random: openssl rand -base64 24

# === Auth.js / NextAuth ===
AUTH_SECRET=                          # Generate: openssl rand -base64 32
AUTH_GOOGLE_ID=                       # Google Cloud Console -> OAuth 2.0 Client ID
AUTH_GOOGLE_SECRET=                   # Google Cloud Console -> OAuth 2.0 Client Secret

# === Caddy / DNS ===
HETZNER_DNS_API_TOKEN=               # Hetzner DNS Console -> API Tokens (Zone.DNS:Edit)

# === AI Services ===
ANTHROPIC_API_KEY=                   # Anthropic Console -> API Keys
VOYAGE_API_KEY=                      # Voyage AI Dashboard -> API Keys

# === R2 Object Storage ===
R2_ENDPOINT=                         # Cloudflare Dashboard -> R2 -> Bucket -> S3 API endpoint
R2_ACCESS_KEY_ID=                    # Cloudflare Dashboard -> R2 -> Manage R2 API Tokens
R2_SECRET_ACCESS_KEY=                # Same as above
R2_BUCKET_NAME=                      # R2 bucket name

# === Admin ===
ADMIN_EMAILS=                        # Comma-separated admin email addresses

# === Backup (set on host, not in Docker) ===
# BACKUP_GPG_PASSPHRASE=             # Symmetric encryption passphrase for backups
# STORAGEBOX_USER=                   # Hetzner Storage Box username (uXXXXXX)
# STORAGEBOX_HOST=                   # Hetzner Storage Box hostname (uXXXXXX.your-storagebox.de)
```

3. Validate the compose file by running:
```bash
cd /home/dev/projects/relay && docker compose -f docker/docker-compose.prod.yml config --quiet 2>&1
```
This will fail on missing env vars but should NOT fail on YAML syntax errors. If it fails, check for YAML syntax issues and fix.

To validate syntax without env vars, use:
```bash
cd /home/dev/projects/relay && DB_PASSWORD=x AUTH_SECRET=x AUTH_GOOGLE_ID=x AUTH_GOOGLE_SECRET=x HETZNER_DNS_API_TOKEN=x ANTHROPIC_API_KEY=x VOYAGE_API_KEY=x R2_ENDPOINT=x R2_ACCESS_KEY_ID=x R2_SECRET_ACCESS_KEY=x R2_BUCKET_NAME=x ADMIN_EMAILS=x docker compose -f docker/docker-compose.prod.yml config --quiet
```
  </action>
  <verify>
Validate compose file syntax:
```bash
cd /home/dev/projects/relay && DB_PASSWORD=x AUTH_SECRET=x AUTH_GOOGLE_ID=x AUTH_GOOGLE_SECRET=x HETZNER_DNS_API_TOKEN=x ANTHROPIC_API_KEY=x VOYAGE_API_KEY=x R2_ENDPOINT=x R2_ACCESS_KEY_ID=x R2_SECRET_ACCESS_KEY=x R2_BUCKET_NAME=x ADMIN_EMAILS=x docker compose -f docker/docker-compose.prod.yml config --quiet
```
Verify key properties:
- `grep "178.156.181.178" /home/dev/projects/relay/docker/docker-compose.prod.yml` (IP-bound ports)
- `grep "pgvector/pgvector:pg17" /home/dev/projects/relay/docker/docker-compose.prod.yml` (correct PG image)
- `grep -c "healthcheck:" /home/dev/projects/relay/docker/docker-compose.prod.yml` should be 3
- `grep "ports:" /home/dev/projects/relay/docker/docker-compose.prod.yml` should appear only once (caddy only)
- Verify .env.example has all env vars referenced in compose: `grep -oP '\$\{(\w+)\}' /home/dev/projects/relay/docker/docker-compose.prod.yml | sort -u`
  </verify>
  <done>
docker-compose.prod.yml defines 3 services with health check cascade. PostgreSQL is internal-only. Caddy binds to public IP only. All env vars documented in .env.example. Compose config validates without YAML errors.
  </done>
</task>

</tasks>

<verification>
- `docker compose -f docker/docker-compose.prod.yml config` validates (with dummy env vars)
- 3 services defined: caddy, web, postgres
- postgres has NO ports section
- caddy ports bind to 178.156.181.178 specifically
- All 3 services have healthcheck definitions
- volumes: postgres_data, caddy_data, caddy_config
- networks: internal (internal: true) and public
- .env.example lists every env var used in compose file
</verification>

<success_criteria>
docker-compose.prod.yml orchestrates Caddy + Next.js + PostgreSQL with health checks, network isolation, and named volumes. .env.example documents all configuration. Compose validates without syntax errors. SOC2-07 (internal-only DB) and DEPLOY-01 through DEPLOY-06 requirements addressed.
</success_criteria>

<output>
After completion, create `.planning/phases/27-production-docker-deployment/27-04-SUMMARY.md`
</output>
