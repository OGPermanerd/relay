---
phase: 38-conversational-mcp-discovery
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - packages/db/src/services/semantic-search.ts
  - packages/db/src/services/index.ts
  - apps/mcp/src/lib/ollama.ts
autonomous: true

must_haves:
  truths:
    - "semanticSearchSkills() returns published skills ranked by cosine similarity when given a valid query embedding"
    - "semanticSearchSkills() returns empty array when no embeddings exist (graceful degradation)"
    - "generateEmbedding() in MCP app returns number[] from Ollama or null on failure"
  artifacts:
    - path: "packages/db/src/services/semantic-search.ts"
      provides: "Semantic vector search with cosine distance"
      exports: ["semanticSearchSkills", "SemanticSearchResult"]
    - path: "apps/mcp/src/lib/ollama.ts"
      provides: "Ollama embedding client for MCP app"
      exports: ["generateEmbedding", "OllamaEmbedConfig"]
  key_links:
    - from: "packages/db/src/services/semantic-search.ts"
      to: "packages/db/src/schema/skill-embeddings.ts"
      via: "innerJoin on skillEmbeddings.skillId = skills.id"
      pattern: "innerJoin.*skillEmbeddings"
    - from: "packages/db/src/services/semantic-search.ts"
      to: "drizzle-orm/sql/functions/vector"
      via: "cosineDistance import"
      pattern: "cosineDistance"
---

<objective>
Create the semantic search service and MCP Ollama client -- the two foundational pieces all discovery tools depend on.

Purpose: recommend_skills needs vector similarity search (plan 02) and describe_skill needs similar-skill lookup (plan 03). Both need these services to exist first.
Output: `semantic-search.ts` service + `ollama.ts` MCP client
</objective>

<execution_context>
@/home/dev/.claude/get-shit-done/workflows/execute-plan.md
@/home/dev/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@packages/db/src/services/search-skills.ts
@packages/db/src/schema/skill-embeddings.ts
@packages/db/src/schema/skills.ts
@apps/web/lib/ollama.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create semantic search service</name>
  <files>packages/db/src/services/semantic-search.ts, packages/db/src/services/index.ts</files>
  <action>
Create `packages/db/src/services/semantic-search.ts` with:

1. Export interface `SemanticSearchResult`:
   - id: string, name: string, slug: string, description: string
   - category: string, totalUses: number, averageRating: number | null
   - similarity: number (0-1, higher = more similar; computed as 1 - cosineDistance)

2. Export async function `semanticSearchSkills(params)`:
   - Params: `{ queryEmbedding: number[]; limit?: number; category?: string; tenantId?: string }`
   - Guard: if (!db) return []
   - Convert queryEmbedding to pgvector string format: `` `[${queryEmbedding.join(",")}]` ``
   - Use `cosineDistance` from `drizzle-orm/sql/functions/vector` (NOT from `drizzle-orm` root -- that import does NOT exist)
   - `const distance = cosineDistance(skillEmbeddings.embedding, vectorStr)`
   - Build conditions array: always include `eq(skills.status, "published")` (DISC-06). Add category and tenantId filters if provided.
   - Query: `db.select({...}).from(skillEmbeddings).innerJoin(skills, eq(skillEmbeddings.skillId, skills.id)).where(and(...conditions)).orderBy(distance).limit(limit)`
   - Select fields: id, name, slug, description, category, totalUses, averageRating from skills, plus distance
   - Map results: compute `similarity: 1 - Number(r.distance)`, strip distance from output
   - Default limit: 10

Import chain:
- `import { sql, eq, and } from "drizzle-orm"`
- `import { cosineDistance } from "drizzle-orm/sql/functions/vector"`
- `import { db } from "../client"`
- `import { skills } from "../schema/skills"`
- `import { skillEmbeddings } from "../schema/skill-embeddings"`

Add to `packages/db/src/services/index.ts`:
```
export { semanticSearchSkills, type SemanticSearchResult } from "./semantic-search";
```
  </action>
  <verify>Run `cd /home/dev/projects/relay && pnpm --filter @everyskill/db exec tsc --noEmit` to confirm no type errors in the db package.</verify>
  <done>semanticSearchSkills is exported, compiles cleanly, queries skill_embeddings with cosine distance and published-only filter</done>
</task>

<task type="auto">
  <name>Task 2: Create Ollama embedding client for MCP app</name>
  <files>apps/mcp/src/lib/ollama.ts</files>
  <action>
Create `apps/mcp/src/lib/ollama.ts` -- a minimal copy of the embedding function from `apps/web/lib/ollama.ts`. The MCP app must be self-contained (no cross-app imports).

1. Export interface `OllamaEmbedConfig { url: string; model: string; }`

2. Export `OLLAMA_DEFAULTS` const:
   - `url: "http://localhost:11434"`
   - `model: "nomic-embed-text"`

3. Export async function `generateEmbedding(text: string, config: OllamaEmbedConfig): Promise<number[] | null>`:
   - try/catch wrapper returning null on any failure
   - AbortController with 5000ms timeout
   - POST to `${config.url}/api/embed` with body `{ model: config.model, input: text }`
   - clearTimeout on response
   - Return null if !response.ok
   - Parse response JSON, extract `data?.embeddings`
   - Return `embeddings[0]` if array with length > 0, else null
   - CRITICAL: No console.log anywhere -- only console.error for debug logging (stdio protocol safety)

Create the `apps/mcp/src/lib/` directory first (it does not exist yet).
  </action>
  <verify>Run `cd /home/dev/projects/relay/apps/mcp && npx tsc --noEmit 2>&1 | grep -v "packages/db"` -- ollama.ts should have zero errors (ignore pre-existing packages/db resolution errors).</verify>
  <done>generateEmbedding and OLLAMA_DEFAULTS exported from apps/mcp/src/lib/ollama.ts, no console.log usage, 5s timeout</done>
</task>

</tasks>

<verification>
- `pnpm --filter @everyskill/db exec tsc --noEmit` passes (semantic-search.ts compiles)
- `apps/mcp/src/lib/ollama.ts` exists and exports generateEmbedding + OLLAMA_DEFAULTS
- semanticSearchSkills includes `eq(skills.status, "published")` filter
- No console.log in any MCP files
</verification>

<success_criteria>
- Semantic search service compiles and is exported from packages/db
- Ollama client exists in MCP app with hardcoded defaults
- Both are ready for consumption by recommend/describe/guide tools in plans 02-03
</success_criteria>

<output>
After completion, create `.planning/phases/38-conversational-mcp-discovery/38-01-SUMMARY.md`
</output>
