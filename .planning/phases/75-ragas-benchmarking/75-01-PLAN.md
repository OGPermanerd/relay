---
phase: 75-ragas-benchmarking
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - packages/db/src/schema/benchmark-runs.ts
  - packages/db/src/migrations/0043_add_ragas_dimensions.sql
  - apps/web/lib/benchmark-runner.ts
autonomous: true

must_haves:
  truths:
    - "Running a benchmark produces faithfulness, relevancy, precision, and recall scores (0-100) for each model-test-case result"
    - "Existing benchmark results with NULL dimension scores remain valid and insertable"
    - "The judge evaluates all 4 dimensions in a single API call (not 4 separate calls)"
    - "The overall qualityScore is NOT recalculated from dimensions -- it remains the judge's holistic score"
  artifacts:
    - path: "packages/db/src/schema/benchmark-runs.ts"
      provides: "4 nullable INTEGER columns on benchmarkResults"
      contains: "faithfulnessScore"
    - path: "packages/db/src/migrations/0043_add_ragas_dimensions.sql"
      provides: "ALTER TABLE migration for 4 dimension columns"
      contains: "faithfulness_score"
    - path: "apps/web/lib/benchmark-runner.ts"
      provides: "Extended judge prompt, schema, and JudgeOutput with 4 dimension fields"
      contains: "faithfulnessScore"
  key_links:
    - from: "apps/web/lib/benchmark-runner.ts"
      to: "packages/db/src/services/benchmark.ts"
      via: "insertBenchmarkResult() call passing dimension scores"
      pattern: "faithfulnessScore.*judge"
    - from: "apps/web/lib/benchmark-runner.ts"
      to: "JUDGE_JSON_SCHEMA"
      via: "output_config json_schema"
      pattern: "faithfulnessScore.*number"
---

<objective>
Add 4-dimension RAGAS scoring to the benchmark schema and judge.

Purpose: Enable per-dimension quality insight (faithfulness, relevancy, precision, recall) for each benchmark result, alongside the existing overall quality score. This is the data foundation for all subsequent UI work.

Output: Migration 0043 adding 4 nullable INTEGER columns, extended Drizzle schema, and extended judge prompt that scores all 4 dimensions in a single structured output call.
</objective>

<execution_context>
@/home/dev/.claude/get-shit-done/workflows/execute-plan.md
@/home/dev/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@packages/db/src/schema/benchmark-runs.ts
@apps/web/lib/benchmark-runner.ts
@packages/db/src/services/benchmark.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Schema migration and Drizzle columns</name>
  <files>
    packages/db/src/schema/benchmark-runs.ts
    packages/db/src/migrations/0043_add_ragas_dimensions.sql
  </files>
  <action>
1. Create migration `packages/db/src/migrations/0043_add_ragas_dimensions.sql`:
   ```sql
   ALTER TABLE benchmark_results ADD COLUMN faithfulness_score INTEGER;
   ALTER TABLE benchmark_results ADD COLUMN relevancy_score INTEGER;
   ALTER TABLE benchmark_results ADD COLUMN precision_score INTEGER;
   ALTER TABLE benchmark_results ADD COLUMN recall_score INTEGER;
   ```
   All columns NULLABLE for backward compatibility (BENCH-05). No DEFAULT, no NOT NULL.

2. Add 4 fields to the `benchmarkResults` table definition in `packages/db/src/schema/benchmark-runs.ts`, after the existing `qualityNotes` field:
   ```typescript
   faithfulnessScore: integer("faithfulness_score"),  // 0-100, RAGAS faithfulness
   relevancyScore: integer("relevancy_score"),        // 0-100, RAGAS answer relevancy
   precisionScore: integer("precision_score"),        // 0-100, RAGAS context precision
   recallScore: integer("recall_score"),              // 0-100, RAGAS context recall
   ```
   Keep all existing fields unchanged. The `BenchmarkResult` and `NewBenchmarkResult` inferred types will automatically include the new nullable fields.

3. Run the migration: `cd /home/dev/projects/relay && pnpm db:migrate`
  </action>
  <verify>
    - `pnpm db:migrate` completes without errors
    - `pnpm turbo typecheck` passes (schema types updated correctly)
    - Query `SELECT column_name FROM information_schema.columns WHERE table_name='benchmark_results' AND column_name LIKE '%_score'` returns 5 columns (quality_score + 4 new)
  </verify>
  <done>benchmark_results table has 4 new nullable INTEGER columns. Existing rows unaffected (all NULLs). Drizzle schema types include the new fields.</done>
</task>

<task type="auto">
  <name>Task 2: Extend judge prompt, schema, and result storage</name>
  <files>
    apps/web/lib/benchmark-runner.ts
  </files>
  <action>
1. Extend the `JudgeOutput` interface to include 4 dimension scores:
   ```typescript
   interface JudgeOutput {
     qualityScore: number;
     qualityNotes: string;
     matchesExpected: boolean;
     faithfulnessScore: number;  // 0-100
     relevancyScore: number;     // 0-100
     precisionScore: number;     // 0-100
     recallScore: number;        // 0-100
   }
   ```

2. Extend `JUDGE_JSON_SCHEMA` with the 4 new fields:
   ```typescript
   const JUDGE_JSON_SCHEMA = {
     type: "object" as const,
     properties: {
       qualityScore: { type: "number" as const },
       qualityNotes: { type: "string" as const },
       matchesExpected: { type: "boolean" as const },
       faithfulnessScore: { type: "number" as const },
       relevancyScore: { type: "number" as const },
       precisionScore: { type: "number" as const },
       recallScore: { type: "number" as const },
     },
     required: [
       "qualityScore", "qualityNotes", "matchesExpected",
       "faithfulnessScore", "relevancyScore", "precisionScore", "recallScore"
     ],
     additionalProperties: false,
   };
   ```

3. Rewrite the `judgeQuality` function's prompt and system message. Add the `skillContent` parameter (currently not passed -- need to thread it through):

   Update `judgeQuality` signature to accept `skillContent: string` as the first parameter:
   ```typescript
   async function judgeQuality(
     client: Anthropic,
     skillContent: string,
     output: string,
     expectedOutput: string | null,
     testInput: string
   ): Promise<JudgeOutput>
   ```

   Update the system message:
   ```
   You are an impartial quality evaluator for AI skill outputs. Score the output on 5 dimensions (0-100 each). Each dimension MUST be scored independently -- it is expected and correct for scores to differ by 20+ points. Do not anchor one score on another.
   ```

   Update the user prompt to include skill content and per-dimension rubric:
   ```
   Evaluate the following output produced by an AI skill.

   <skill_instructions>
   ${skillContent}
   </skill_instructions>

   <input>
   ${testInput}
   </input>

   <output>
   ${output}
   </output>${expectedSection}

   Score the output on FIVE dimensions (0-100 each). Each dimension MUST be scored independently.

   1. OVERALL QUALITY (qualityScore): Holistic assessment of correctness, completeness, and usefulness.

   2. FAITHFULNESS (faithfulnessScore): Does the output follow the skill instructions without hallucinating, deviating, or adding unsupported claims? 100 = perfectly faithful to instructions. 0 = completely ignores or contradicts instructions.

   3. RELEVANCY (relevancyScore): Is the output relevant to the specific input provided? 100 = directly and completely addresses the input. 0 = output has nothing to do with the input.

   4. PRECISION (precisionScore): Did the model use the skill instructions effectively? 100 = every part of the skill was leveraged appropriately. 0 = skill instructions were completely ignored.

   5. RECALL (recallScore): Does the output address ALL aspects of the input? 100 = every aspect of the input is covered. 0 = major aspects of the input were missed.

   Example of valid divergent scoring:
   - An output that perfectly follows instructions (faithfulness=95) but only addresses half the input (recall=50)
   - An output that covers everything in the input (recall=90) but ignores the skill format requirements (precision=40)
   - A thorough, relevant response (relevancy=90, recall=85) that adds claims not in the skill instructions (faithfulness=55)
   ```

4. Increase `max_tokens` from 512 to 1024 in the judge call (Pitfall 5 from research).

5. Update the fallback return in the catch block and the "No judge response" fallback to include 0 values for all 4 dimension scores:
   ```typescript
   return {
     qualityScore: 0,
     qualityNotes: "...",
     matchesExpected: false,
     faithfulnessScore: 0,
     relevancyScore: 0,
     precisionScore: 0,
     recallScore: 0,
   };
   ```

6. Update the `judgeQuality` call site in `runBenchmark` to pass `params.skillContent`:
   ```typescript
   judge = await judgeQuality(
     client,
     params.skillContent,
     result.outputText,
     testCase.expectedOutput,
     testCase.input
   );
   ```

7. Update the `insertBenchmarkResult` call (the success path, around line 247) to include dimension scores:
   ```typescript
   await insertBenchmarkResult({
     ...existing fields...,
     faithfulnessScore: judge.faithfulnessScore,
     relevancyScore: judge.relevancyScore,
     precisionScore: judge.precisionScore,
     recallScore: judge.recallScore,
   });
   ```
   The error path (line 214) needs NO changes -- the dimension fields are nullable, so omitting them is correct.

IMPORTANT: Do NOT modify `qualityScore` calculation or the summary stats computation (bestModel/cheapestModel). Those remain exactly as-is per research anti-pattern guidance.
  </action>
  <verify>
    - `pnpm turbo typecheck` passes
    - `pnpm lint` passes (no unused variables, no-console warnings OK)
    - Manually verify: JudgeOutput has 7 fields, JUDGE_JSON_SCHEMA has 7 properties, judge prompt mentions all 5 dimensions, max_tokens is 1024
  </verify>
  <done>The benchmark runner produces 4 dimension scores alongside the overall quality score for each judge evaluation. Dimension scores are stored in the database via insertBenchmarkResult. Existing error paths gracefully handle missing dimensions (nullable columns). The judge prompt includes divergent scoring examples to mitigate score clustering (Pitfall 1).</done>
</task>

</tasks>

<verification>
- `pnpm turbo typecheck` passes across the monorepo
- `pnpm lint` passes
- Migration 0043 applied successfully
- Schema types include faithfulnessScore, relevancyScore, precisionScore, recallScore as optional integers
- benchmark-runner.ts compiles with extended JudgeOutput and JUDGE_JSON_SCHEMA
- No changes to qualityScore computation or bestModel/cheapestModel summary logic
</verification>

<success_criteria>
1. The benchmark_results table has 4 new nullable INTEGER columns (faithfulness_score, relevancy_score, precision_score, recall_score)
2. Running a benchmark will produce per-dimension scores stored alongside the overall quality score
3. Existing benchmark results (with NULL dimension scores) remain valid -- no errors, no data migration needed
4. The judge evaluates all dimensions in a single API call with a prompt designed to produce divergent scores
</success_criteria>

<output>
After completion, create `.planning/phases/75-ragas-benchmarking/75-01-SUMMARY.md`
</output>
