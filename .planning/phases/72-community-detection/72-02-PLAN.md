---
phase: 72-community-detection
plan: 02
type: execute
wave: 2
depends_on: ["72-01"]
files_modified:
  - packages/db/src/services/community-detection.ts
  - packages/db/src/services/index.ts
  - apps/web/app/api/cron/community-detection/route.ts
autonomous: true

must_haves:
  truths:
    - "Running detection on the default tenant produces distinct communities from real skill embeddings"
    - "Community assignments are persisted in skill_communities table with correct tenant_id"
    - "Detection with fewer than 20 published skill embeddings returns gracefully without crashing"
    - "Cron endpoint authenticates with CRON_SECRET and triggers detection for a given tenant"
    - "Refresh replaces old communities atomically (transaction: delete + insert)"
  artifacts:
    - path: "packages/db/src/services/community-detection.ts"
      provides: "KNN edge extraction, graphology Louvain clustering, atomic persist"
      exports: ["detectCommunities"]
    - path: "apps/web/app/api/cron/community-detection/route.ts"
      provides: "GET endpoint with CRON_SECRET auth, tenantId query param"
      exports: ["GET"]
  key_links:
    - from: "packages/db/src/services/community-detection.ts"
      to: "packages/db/src/schema/skill-communities.ts"
      via: "import skillCommunities for insert"
      pattern: "skillCommunities"
    - from: "packages/db/src/services/community-detection.ts"
      to: "skill_embeddings table"
      via: "LATERAL JOIN KNN query"
      pattern: "LATERAL"
    - from: "apps/web/app/api/cron/community-detection/route.ts"
      to: "packages/db/src/services/community-detection.ts"
      via: "import detectCommunities"
      pattern: "detectCommunities"
---

<objective>
Implement the community detection algorithm service and cron endpoint. The service fetches KNN edges from pgvector via LATERAL JOIN, builds an in-memory graph with graphology, runs Louvain clustering, and persists results atomically. The cron endpoint triggers detection for a tenant.

Purpose: This is the core algorithm that fulfills COMM-01 (cluster skills into thematic communities) and COMM-05 (persist and refresh via cron).
Output: Working community detection callable via cron, with real community assignments in the database.
</objective>

<execution_context>
@/home/dev/.claude/get-shit-done/workflows/execute-plan.md
@/home/dev/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/72-community-detection/72-RESEARCH.md (KNN query, Louvain code, cron pattern, pitfalls)
@.planning/phases/72-community-detection/72-01-SUMMARY.md
@packages/db/src/services/hybrid-search.ts (db.execute(sql`...`) pattern for raw SQL)
@apps/web/app/api/cron/integrity-check/route.ts (cron endpoint with query params pattern)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Community detection service with KNN + Louvain + atomic persist</name>
  <files>
    packages/db/src/services/community-detection.ts
    packages/db/src/services/index.ts
  </files>
  <action>
    Create `packages/db/src/services/community-detection.ts` with:

    1. Constants at top:
       - K = 10 (nearest neighbors per skill)
       - MIN_SIMILARITY = 0.3 (edge threshold)
       - MIN_SKILLS_FOR_DETECTION = 5 (below this, skip gracefully)
       - RESOLUTION = 1.0 (Louvain resolution parameter)

    2. `detectCommunities(tenantId: string)` async function that returns `{ communities: number; modularity: number; skills: number; edges: number; skipped?: string }`:

       a. **Count eligible skills first:** Run a quick count query joining skill_embeddings with skills where status='published' AND visibility IN ('global_approved', 'tenant') AND tenant_id = tenantId. If count < MIN_SKILLS_FOR_DETECTION, return `{ communities: 0, modularity: 0, skills: count, edges: 0, skipped: "Too few skills for community detection" }`.

       b. **KNN edge extraction:** Use `db.execute(sql`...`)` with LATERAL JOIN pattern from research:
          - FROM skill_embeddings a JOIN skills sa ON sa.id = a.skill_id
          - JOIN LATERAL (SELECT b.skill_id, b.embedding FROM skill_embeddings b JOIN skills sb ... WHERE b.skill_id != a.skill_id AND b.tenant_id = a.tenant_id AND sb.status = 'published' AND sb.visibility IN ('global_approved', 'tenant') ORDER BY a.embedding <=> b.embedding LIMIT K) nn ON true
          - WHERE a.tenant_id = tenantId AND sa.status = 'published' AND sa.visibility IN ('global_approved', 'tenant') AND (1 - (a.embedding <=> nn.embedding)) >= MIN_SIMILARITY
          - SELECT a.skill_id AS source_id, nn.skill_id AS target_id, (1 - (a.embedding <=> nn.embedding))::float AS similarity

       c. **Build graph:** Import `{ UndirectedGraph } from 'graphology'` and `louvain from 'graphology-communities-louvain'`. Create UndirectedGraph, iterate edges calling `graph.mergeEdge(sourceId, targetId, { weight: similarity })`.

       d. **Run Louvain:** If graph.order < 3, return with skipped message. Otherwise call `louvain.detailed(graph, { resolution: RESOLUTION })`. Check if `details.count <= 1` or `details.modularity < 0.1` -- if so, log warning but still persist (single community is valid for small/homogeneous datasets).

       e. **Persist atomically:** Within a `db.transaction(async (tx) => { ... })`:
          - DELETE FROM skill_communities WHERE tenant_id = tenantId (use `tx.delete(skillCommunities).where(eq(skillCommunities.tenantId, tenantId))`)
          - Build assignments array: for each [skillId, communityIndex] in details.communities, create `{ tenantId, skillId, communityId: communityIndex, modularity: details.modularity, detectedAt: new Date() }`
          - Bulk insert via `tx.insert(skillCommunities).values(assignments)` (only if assignments.length > 0)

       f. Return `{ communities: details.count, modularity: details.modularity, skills: graph.order, edges: graph.size }`.

    3. Import patterns: Use `import { db } from "../client"` and `import { sql, eq } from "drizzle-orm"` and `import { skillCommunities } from "../schema"`. For graphology use named import `{ UndirectedGraph }` (NOT default import -- see Pitfall 6 in research).

    4. Add export to `packages/db/src/services/index.ts`:
       `export { detectCommunities } from "./community-detection";`

    5. Run typecheck: `cd /home/dev/projects/relay && pnpm turbo typecheck`

    IMPORTANT: The raw SQL in the LATERAL JOIN uses template literals via drizzle's `sql` tag. Use `sql.raw()` for any string identifiers if needed, but prefer parameterized values for tenantId, K, and MIN_SIMILARITY. The cosine distance operator is `<=>` which is plain SQL, not a JS operator.
  </action>
  <verify>
    - `pnpm turbo typecheck` passes
    - Import resolves: `node -e "import('@everyskill/db').then(m => console.log(typeof m.detectCommunities))"` prints "function"
  </verify>
  <done>
    detectCommunities function exists, type-checks, and is exported from @everyskill/db barrel. Handles graceful fallback for small datasets. Uses LATERAL JOIN KNN (not CROSS JOIN). Persists atomically via transaction.
  </done>
</task>

<task type="auto">
  <name>Task 2: Cron endpoint and live verification</name>
  <files>
    apps/web/app/api/cron/community-detection/route.ts
  </files>
  <action>
    1. Create `apps/web/app/api/cron/community-detection/route.ts` following the integrity-check cron pattern:
       - Export async function GET(request: NextRequest)
       - CRON_SECRET check: if not configured, return `{ skipped: true, reason: "CRON_SECRET not configured" }`
       - Auth header check: if mismatch, return 401
       - Read optional `tenantId` query param: `request.nextUrl.searchParams.get("tenantId")` with fallback to DEFAULT_TENANT_ID ("default-tenant-000-0000-000000000000")
       - Call `detectCommunities(tenantId)` from `@everyskill/db`
       - Return the result as JSON with 200
       - Wrap in try/catch, log errors with `[COMMUNITY DETECTION]` prefix, return 500 on failure

    2. Verify the full pipeline by running detection against the real database:
       - First check CRON_SECRET exists: `grep CRON_SECRET /home/dev/projects/relay/apps/web/.env.local`
       - Build: `cd /home/dev/projects/relay && pnpm build`
       - If dev server running, call endpoint directly:
         `curl -s -H "Authorization: Bearer $CRON_SECRET" "http://localhost:2002/api/cron/community-detection" | jq .`
       - Verify communities were persisted: `psql -d everyskill -c "SELECT community_id, COUNT(*) as members FROM skill_communities GROUP BY community_id ORDER BY community_id"`

    3. If the curl test reveals issues (import errors, SQL errors), fix them in the service or endpoint and re-test.

    4. Final lint check: `cd /home/dev/projects/relay && pnpm lint`
  </action>
  <verify>
    - `pnpm build` passes (confirms endpoint compiles in production build)
    - Curl to cron endpoint returns 200 with `{ communities: N, modularity: M, skills: S, edges: E }` where N >= 2 and S > 0
    - `psql -d everyskill -c "SELECT COUNT(*) FROM skill_communities"` returns > 0 rows
    - `psql -d everyskill -c "SELECT community_id, COUNT(*) FROM skill_communities GROUP BY community_id ORDER BY community_id"` shows multiple distinct communities
    - `pnpm lint` passes
  </verify>
  <done>
    Cron endpoint at /api/cron/community-detection works end-to-end. Running it against real data produces multiple distinct communities persisted in skill_communities. Communities can be refreshed by calling the endpoint again (atomic replace). Endpoint accepts optional tenantId query param. Build and lint pass.
  </done>
</task>

</tasks>

<verification>
Phase 72 success criteria verification:

1. "Running community detection on a tenant with 20+ published skills produces distinct communities where each community's member skills share thematic similarity (average intra-community cosine similarity > 0.5)"
   - Verify: `curl -s -H "Authorization: Bearer $CRON_SECRET" "http://localhost:2002/api/cron/community-detection"` returns communities > 1
   - Verify: `psql -d everyskill -c "SELECT community_id, COUNT(*) FROM skill_communities GROUP BY community_id"` shows multiple communities with varying sizes

2. "Community assignments are persisted in the database with tenant isolation and can be refreshed via a cron endpoint without affecting live queries"
   - Verify: skill_communities table has tenant_id on all rows, RLS policy exists
   - Verify: Running the endpoint twice produces consistent results (second run replaces first atomically)

3. "Running detection on a tenant with fewer than 20 published skills or a sparse graph gracefully falls back"
   - Verify: The detectCommunities function returns `{ skipped: "Too few skills..." }` for small datasets without throwing
</verification>

<success_criteria>
- detectCommunities service runs KNN via LATERAL JOIN (not CROSS JOIN), clusters via graphology Louvain, persists atomically
- Cron endpoint at /api/cron/community-detection authenticates with CRON_SECRET and triggers detection
- Real data test produces 2+ distinct communities from 145 published skills
- Graceful fallback for tenants with < 5 eligible skills
- Build, typecheck, and lint all pass
</success_criteria>

<output>
After completion, create `.planning/phases/72-community-detection/72-02-SUMMARY.md`
</output>
