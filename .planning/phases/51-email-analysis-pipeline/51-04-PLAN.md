---
phase: 51-email-analysis-pipeline
plan: 04
type: execute
wave: 2
depends_on: [51-01, 51-02, 51-03]
files_modified:
  - apps/web/lib/email-time-estimator.ts
  - apps/web/lib/diagnostic-aggregator.ts
  - apps/web/app/actions/email-diagnostic.ts
autonomous: true

must_haves:
  truths:
    - "Time estimation applies configurable per-category weights (minutes per email)"
    - "Aggregator computes category breakdown with counts, percentages, time estimates"
    - "Aggregator derives pattern insights (busiest hour, busiest day) from timestamps"
    - "Server action orchestrates fetch -> classify -> estimate -> aggregate -> save, all in memory"
  artifacts:
    - path: "apps/web/lib/email-time-estimator.ts"
      provides: "estimateTimeSpent and estimateHoursPerWeek functions"
      exports: ["estimateTimeSpent", "estimateHoursPerWeek", "DEFAULT_TIME_WEIGHTS"]
    - path: "apps/web/lib/diagnostic-aggregator.ts"
      provides: "computeAggregates function"
      exports: ["computeAggregates", "AggregateResults"]
    - path: "apps/web/app/actions/email-diagnostic.ts"
      provides: "runEmailDiagnostic server action"
      exports: ["runEmailDiagnostic"]
  key_links:
    - from: "apps/web/app/actions/email-diagnostic.ts"
      to: "apps/web/lib/gmail-client.ts"
      via: "import fetchEmailMetadata"
      pattern: "fetchEmailMetadata"
    - from: "apps/web/app/actions/email-diagnostic.ts"
      to: "apps/web/lib/email-classifier.ts"
      via: "import classifyEmails"
      pattern: "classifyEmails"
    - from: "apps/web/app/actions/email-diagnostic.ts"
      to: "@everyskill/db/services/email-diagnostics"
      via: "import saveEmailDiagnostic"
      pattern: "saveEmailDiagnostic"
---

<objective>
Create time estimation, aggregation, and orchestration layer that processes email metadata entirely in memory.

Purpose: Complete the analyze-and-discard pipeline. Orchestrator fetches metadata, classifies, estimates time, computes aggregates, saves ONLY statistics, then discards raw data. Everything in-memory within a single server action.
Output: Time estimator, aggregator, and runEmailDiagnostic server action.
</objective>

<execution_context>
@/home/dev/.claude/get-shit-done/workflows/execute-plan.md
@/home/dev/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@apps/web/lib/gmail-client.ts
@apps/web/lib/email-classifier.ts
@packages/db/src/services/email-diagnostics.ts
@apps/web/app/actions/skills.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create time estimation and aggregation utilities</name>
  <files>
apps/web/lib/email-time-estimator.ts
apps/web/lib/diagnostic-aggregator.ts
  </files>
  <action>
Create `apps/web/lib/email-time-estimator.ts`:

Import EmailCategory and ClassifiedEmail from "./email-classifier"

Export const DEFAULT_TIME_WEIGHTS: Record<EmailCategory, number>:
```
{
  "newsletter": 0.5,
  "automated-notification": 0.3,
  "meeting-invite": 2.0,
  "direct-message": 3.0,
  "internal-thread": 4.0,
  "vendor-external": 3.5,
  "support-ticket": 5.0
}
```

Export interface TimeEstimate:
- category: EmailCategory
- count: number
- estimatedMinutes: number

Export function estimateTimeSpent(classified: ClassifiedEmail[], customWeights?: Partial<typeof DEFAULT_TIME_WEIGHTS>): TimeEstimate[]
Implementation:
1. Merge DEFAULT_TIME_WEIGHTS with customWeights (if provided)
2. Group by category using Map<EmailCategory, number>
3. Calculate estimated minutes: count * weight for each category
4. Return TimeEstimate array

Export function estimateHoursPerWeek(estimates: TimeEstimate[], scanPeriodDays: number): number
Implementation:
1. Sum totalMinutes from all estimates
2. Calculate minutesPerDay: totalMinutes / scanPeriodDays
3. Calculate hoursPerWeek: (minutesPerDay * 7) / 60
4. Round to 1 decimal: Math.round(hoursPerWeek * 10) / 10

---

Create `apps/web/lib/diagnostic-aggregator.ts`:

Import ClassifiedEmail and EmailCategory from "./email-classifier"
Import DEFAULT_TIME_WEIGHTS from "./email-time-estimator"

Export interface AggregateResults:
- categoryBreakdown: Array<{category: string, count: number, percentage: number, estimatedMinutes: number}>
- patternInsights: {busiestHour: number, busiestDayOfWeek: string, averageResponseTimeHours: number | null, threadDepthAverage: number}
- estimatedHoursPerWeek: number

Export function computeAggregates(classified: ClassifiedEmail[], scanPeriodDays: number): AggregateResults
Implementation:
1. Category stats:
   - Build Map<string, {count: number, estimatedMinutes: number}>
   - For each email: increment count, add weight from DEFAULT_TIME_WEIGHTS
   - Calculate percentages: (count / totalMessages) * 100
   - Build categoryBreakdown array
2. Pattern insights:
   - Hours array (24 elements): count emails per hour using email.date.getHours()
   - Days map: count emails per day using email.date.getDay() mapped to day name
   - busiestHour: hours.indexOf(Math.max(...hours))
   - busiestDayOfWeek: day with max count
   - averageResponseTimeHours: null (TODO for future)
   - threadDepthAverage: 0 (TODO for future)
3. Hours per week:
   - totalMinutes = sum of categoryBreakdown estimatedMinutes
   - minutesPerDay = totalMinutes / scanPeriodDays
   - hoursPerWeek = (minutesPerDay * 7) / 60
   - Round to 1 decimal
4. Return AggregateResults

Day names array: ["Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"]
  </action>
  <verify>
grep "export.*estimateTimeSpent" apps/web/lib/email-time-estimator.ts
grep "DEFAULT_TIME_WEIGHTS" apps/web/lib/email-time-estimator.ts
grep "export.*computeAggregates" apps/web/lib/diagnostic-aggregator.ts
  </verify>
  <done>
Time estimator exports estimateTimeSpent and estimateHoursPerWeek with DEFAULT_TIME_WEIGHTS. Aggregator exports computeAggregates with category breakdown and pattern insights.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create email diagnostic server action</name>
  <files>
apps/web/app/actions/email-diagnostic.ts
  </files>
  <action>
Create `apps/web/app/actions/email-diagnostic.ts`:

"use server" directive at top.

Import:
- auth from "@/auth"
- fetchEmailMetadata from "@/lib/gmail-client"
- classifyEmails from "@/lib/email-classifier"
- computeAggregates from "@/lib/diagnostic-aggregator"
- saveEmailDiagnostic from "@everyskill/db/services/email-diagnostics"
- GmailNotConnectedError, GmailTokenRevokedError from "@everyskill/db/services/gmail-tokens"

Export async function runEmailDiagnostic(): Promise<{success: boolean, data?: AggregateResults, error?: string}>

Implementation:
1. Get session: `const session = await auth()`
2. If no session or no userId or no tenantId, return {success: false, error: "Unauthorized"}
3. Wrap in try/catch:
   - Fetch metadata: `const messages = await fetchEmailMetadata(session.user.id, {daysBack: 90, maxMessages: 5000})`
   - Classify: `const classified = await classifyEmails(messages)`
   - Aggregate: `const aggregates = computeAggregates(classified, 90)`
   - Store results ONLY (aggregates, not raw metadata):
     ```
     await saveEmailDiagnostic({
       userId: session.user.id,
       tenantId: session.user.tenantId!,
       scanDate: new Date(),
       scanPeriodDays: 90,
       totalMessages: messages.length,
       categoryBreakdown: aggregates.categoryBreakdown,
       estimatedHoursPerWeek: Math.round(aggregates.estimatedHoursPerWeek * 10), // stored as tenths
       patternInsights: aggregates.patternInsights
     })
     ```
   - Return {success: true, data: aggregates}
4. Catch block:
   - If GmailNotConnectedError: return {success: false, error: "Gmail account not connected. Please connect your Gmail account first."}
   - If GmailTokenRevokedError: return {success: false, error: "Gmail access has been revoked. Please reconnect your Gmail account."}
   - Otherwise: return {success: false, error: "Failed to analyze emails. Please try again."}

IMPORTANT: Raw metadata (messages array) is ONLY in memory. After saveEmailDiagnostic, it's garbage-collected. Only aggregates are persisted.

Follow existing server action patterns from skills.ts (error handling, session check, return shape).
  </action>
  <verify>
grep '"use server"' apps/web/app/actions/email-diagnostic.ts
grep "runEmailDiagnostic" apps/web/app/actions/email-diagnostic.ts
grep "saveEmailDiagnostic" apps/web/app/actions/email-diagnostic.ts
grep "fetchEmailMetadata" apps/web/app/actions/email-diagnostic.ts
grep "classifyEmails" apps/web/app/actions/email-diagnostic.ts
grep "computeAggregates" apps/web/app/actions/email-diagnostic.ts
  </verify>
  <done>
Server action runEmailDiagnostic orchestrates fetch -> classify -> aggregate -> save pipeline. Returns success/error shape. Raw metadata only in memory.
  </done>
</task>

</tasks>

<verification>
1. Time estimator applies DEFAULT_TIME_WEIGHTS per category
2. Aggregator computes category breakdown with percentages
3. Aggregator derives busiest hour and day from timestamps
4. Server action orchestrates full pipeline in memory
5. Only aggregates stored in database, raw metadata discarded
</verification>

<success_criteria>
- Time estimation applies configurable weights (newsletter 0.5min, direct-message 3min, etc.)
- Aggregator computes category breakdown with counts, percentages, estimated minutes
- Aggregator derives pattern insights (busiest hour 0-23, busiest day name)
- Server action orchestrates fetch -> classify -> estimate -> aggregate -> save entirely in memory
- Raw email metadata never persisted, only aggregate statistics stored
- Error handling for GmailNotConnectedError and GmailTokenRevokedError
</success_criteria>

<output>
After completion, create `.planning/phases/51-email-analysis-pipeline/51-04-SUMMARY.md`
</output>
