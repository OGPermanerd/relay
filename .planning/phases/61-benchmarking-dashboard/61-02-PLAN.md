---
phase: 61-benchmarking-dashboard
plan: 02
type: execute
wave: 2
depends_on: ["61-01"]
files_modified:
  - apps/web/app/actions/benchmark.ts
  - apps/web/components/benchmark-tab.tsx
  - apps/web/components/cost-trend-chart.tsx
autonomous: true

must_haves:
  truths:
    - "Admin can trigger a benchmark run via server action and see progress with elapsed timer"
    - "Benchmark tab shows quick stats (avg cost, avg tokens, quality score, feedback sentiment)"
    - "Cost trend chart renders historical cost data as a Recharts AreaChart"
    - "Model comparison table shows side-by-side metrics when multi-model data exists"
    - "Staleness warning appears when latest benchmark is older than 90 days or never run"
  artifacts:
    - path: "apps/web/app/actions/benchmark.ts"
      provides: "triggerBenchmark server action"
      exports: ["triggerBenchmark", "BenchmarkState"]
    - path: "apps/web/components/benchmark-tab.tsx"
      provides: "Main benchmark tab with stats, comparison table, staleness, trigger button"
      exports: ["BenchmarkTab"]
    - path: "apps/web/components/cost-trend-chart.tsx"
      provides: "Recharts AreaChart for cost over time"
      exports: ["CostTrendChart"]
  key_links:
    - from: "apps/web/app/actions/benchmark.ts"
      to: "apps/web/lib/benchmark-runner.ts"
      via: "runBenchmark call"
      pattern: "import.*runBenchmark.*from.*benchmark-runner"
    - from: "apps/web/components/benchmark-tab.tsx"
      to: "apps/web/app/actions/benchmark.ts"
      via: "useActionState(triggerBenchmark)"
      pattern: "useActionState.*triggerBenchmark"
    - from: "apps/web/components/benchmark-tab.tsx"
      to: "apps/web/components/cost-trend-chart.tsx"
      via: "CostTrendChart component"
      pattern: "CostTrendChart"
---

<objective>
Create the benchmark server action, benchmark tab UI component, and cost trend chart.

Purpose: Wire the execution engine (Plan 01) to a user-facing UI with stats display, model comparison, staleness detection, and async benchmark triggering.

Output: Server action + 2 components (BenchmarkTab, CostTrendChart)
</objective>

<execution_context>
@/home/dev/.claude/get-shit-done/workflows/execute-plan.md
</execution_context>

<context>
@.planning/phases/61-benchmarking-dashboard/61-01-SUMMARY.md
@apps/web/app/actions/ai-review.ts
@apps/web/components/ai-review-tab.tsx
@apps/web/components/stat-card.tsx
@apps/web/components/usage-area-chart.tsx
@apps/web/lib/pricing-table.ts
@apps/web/lib/skill-feedback-stats.ts
@packages/db/src/services/token-measurements.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Benchmark server action and cost trend chart component</name>
  <files>apps/web/app/actions/benchmark.ts, apps/web/components/cost-trend-chart.tsx</files>
  <action>
**Server action: `apps/web/app/actions/benchmark.ts`**

Follow the exact pattern from `apps/web/app/actions/ai-review.ts`.

```typescript
"use server";

export type BenchmarkState = {
  error?: string;
  success?: boolean;
  runId?: string;
};
```

`triggerBenchmark(prevState: BenchmarkState, formData: FormData): Promise<BenchmarkState>`

1. Auth check: `const session = await auth()`, verify `session.user.id` and `session.user.tenantId`
2. Extract `skillId` from formData. Also extract optional `adHocInputs` (JSON string of string[]) for cases where no training examples exist.
3. Verify db exists, fetch skill (id, name, content, slug, authorId)
4. Authorization: admin OR author can trigger benchmarks. Use `isAdmin(session)` from `@/lib/admin` and check `skill.authorId === session.user.id`.
5. Gather test cases:
   a. Call `getTrainingExamples(skillId)` from `@everyskill/db/services`
   b. If training examples exist, use them as test cases (map to `{ input: exampleInput, expectedOutput }`)
   c. If none exist AND `adHocInputs` provided: parse JSON array of strings, map to `{ input, expectedOutput: null }`
   d. If none exist AND no adHocInputs: return `{ error: "No test cases available. Add training examples or provide ad-hoc test inputs." }`
   e. Limit to max 3 test cases (slice first 3)
6. Call `runBenchmark({ tenantId, skillId, triggeredBy: session.user.id, skillContent: skill.content, skillName: skill.name, testCases })`
7. `revalidatePath(\`/skills/${skill.slug}\`)`
8. Return `{ success: true, runId: result.runId }`
9. Wrap in try/catch, return `{ error: "Benchmark service temporarily unavailable..." }` on failure

Do NOT export types from this "use server" file that are used elsewhere -- keep BenchmarkState defined here only for useActionState consumption. Per MEMORY.md: "Don't re-export types from use server files".

**Cost trend chart: `apps/web/components/cost-trend-chart.tsx`**

"use client" component. Follow the exact Recharts pattern from `apps/web/components/usage-area-chart.tsx`.

```typescript
interface CostTrendPoint {
  date: string;
  avgCost: number; // microcents
}

interface CostTrendChartProps {
  data: CostTrendPoint[];
  height?: number;
}
```

- Use `ResponsiveContainer`, `AreaChart`, `Area`, `XAxis`, `YAxis`, `CartesianGrid`, `Tooltip` from recharts
- XAxis tickFormatter: format date as "Mon DD" using manual UTC parsing (NOT toLocaleDateString per MEMORY.md hydration rule). Use: `const d = new Date(date); const months = ["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"]; return months[d.getUTCMonth()] + " " + d.getUTCDate();`
- YAxis tickFormatter: format microcents as dollar string using `formatCostMicrocents` from `@/lib/pricing-table`
- Tooltip formatter: show `formatCostMicrocents(value)` with label "Avg Cost"
- Area color: `stroke="#8b5cf6"` (purple, to distinguish from the blue usage chart), `fill="#8b5cf6"`, `fillOpacity={0.1}`
- Empty state: same pattern as UsageAreaChart -- dashed border box with "No cost data for this period"
  </action>
  <verify>Run `cd /home/dev/projects/relay/apps/web && npx tsc --noEmit` to confirm both files compile.</verify>
  <done>Server action handles auth, test case gathering (training examples or ad-hoc), delegates to runBenchmark, returns result. CostTrendChart renders an AreaChart with proper UTC date formatting and microcent cost display.</done>
</task>

<task type="auto">
  <name>Task 2: BenchmarkTab client component with stats, comparison, staleness, and trigger</name>
  <files>apps/web/components/benchmark-tab.tsx</files>
  <action>
Create `apps/web/components/benchmark-tab.tsx` as a "use client" component.

**Props interface:**
```typescript
interface BenchmarkTabProps {
  skillId: string;
  skillSlug: string;
  isAdmin: boolean;
  isAuthor: boolean;
  // Data from server (serialized)
  latestRun: {
    id: string;
    status: string;
    models: string[];
    bestModel: string | null;
    bestQualityScore: number | null;
    cheapestModel: string | null;
    cheapestCostMicrocents: number | null;
    completedAt: string | null; // ISO string
    createdAt: string;
  } | null;
  modelComparison: {
    modelName: string;
    avgQuality: number;
    avgCost: number;
    avgTokens: number;
    avgLatency: number;
    testCases: number;
  }[];
  costTrendData: { date: string; avgCost: number }[];
  costStats: {
    totalCostMicrocents: number;
    avgCostPerUseMicrocents: number;
    measurementCount: number;
    predominantModel: string | null;
  };
  feedbackStats: {
    totalFeedback: number;
    positivePct: number | null;
  };
  hasTrainingExamples: boolean;
}
```

**Component sections (render in this order):**

1. **Quick Stats Row** (BENCH-02): 4 StatCard components in a 2x2 or 4-col grid.
   - Avg Cost: `formatCostMicrocents(costStats.avgCostPerUseMicrocents)` with label "Avg Cost / Use". Show "N/A" if measurementCount === 0.
   - Avg Tokens: if latestRun?.bestModel, show from modelComparison row. Otherwise show "N/A". Label: "Avg Tokens".
   - Quality Score: `latestRun?.bestQualityScore ?? "N/A"` with suffix "/100". Label: "Best Quality Score".
   - Feedback: `feedbackStats.positivePct !== null ? feedbackStats.positivePct + "%" : "N/A"`. Label: "Positive Feedback".

   Import StatCard from `@/components/stat-card`.

2. **Staleness Detection Banner** (BENCH-08): Show amber warning banner if:
   - `latestRun === null` (never benchmarked) -- message: "This skill has never been benchmarked."
   - OR `latestRun.completedAt` is older than 90 days -- message: "Last benchmark was over 90 days ago. Results may be outdated."

   Calculate staleness: `const isStale = !latestRun?.completedAt || (Date.now() - new Date(latestRun.completedAt).getTime() > 90 * 24 * 60 * 60 * 1000);`

   Banner style: `rounded-lg border border-amber-200 bg-amber-50 p-4` with amber text.
   Include "Re-benchmark" button inside the banner (same as the trigger form below).

3. **Run Benchmark Section** (BENCH-05): Only show if `isAdmin || isAuthor`.

   Use `useActionState(triggerBenchmark, {})` pattern from ai-review-tab.tsx.

   Extract and use the `useElapsedTimer` hook. IMPORTANT: Copy the `useElapsedTimer` hook implementation directly into this file (it's defined locally in ai-review-tab.tsx, not exported). Do NOT import from ai-review-tab.

   Form with hidden skillId input. Submit button: "Run Benchmark" / "Running... {elapsed}s" when pending.

   If `!hasTrainingExamples`: show a textarea for ad-hoc test inputs (one per line). Parse on submit: split by newline, filter empty, JSON.stringify as `adHocInputs` hidden input. Show helper text: "No training examples found. Enter test inputs below (one per line):"

   Show error/success messages from state.

4. **Model Comparison Table** (BENCH-03): Only render if `modelComparison.length > 1`.

   HTML table with columns: Model, Quality (0-100), Avg Cost, Avg Tokens, Latency.
   - Model: display shortened name (strip "claude-" prefix and date suffix pattern "-YYYYMMDD"). Helper: `const shortModel = (name: string) => name.replace(/^claude-/, "").replace(/-\d{8}$/, "");`
   - Quality: `row.avgQuality.toFixed(0)` with color coding (green >= 70, amber >= 40, red < 40)
   - Avg Cost: `formatCostMicrocents(row.avgCost)`
   - Avg Tokens: `row.avgTokens.toFixed(0)`
   - Latency: `${(row.avgLatency / 1000).toFixed(1)}s`

   Highlight best quality row and cheapest row with subtle background colors.
   Table styling: standard tailwind table with `text-sm`, alternating row colors.

5. **Cost Trend Chart** (BENCH-04): Import and render `CostTrendChart` from `./cost-trend-chart` with `costTrendData`. Only render if `costTrendData.length > 0`. Wrap in a section with heading "Cost Trend (Last 90 Days)".

6. **Latest Run Summary**: If `latestRun` exists, show a small footer: "Last benchmarked: {relative time}" and "Models tested: {models.join(", ")}". Use manual UTC date formatting for the timestamp (NOT toLocaleDateString).

Import `formatCostMicrocents` from `@/lib/pricing-table`. Import `triggerBenchmark` from `@/app/actions/benchmark`.
  </action>
  <verify>Run `cd /home/dev/projects/relay/apps/web && npx tsc --noEmit` to confirm the component compiles.</verify>
  <done>BenchmarkTab renders quick stats, staleness detection, benchmark trigger with ad-hoc inputs fallback, model comparison table, cost trend chart, and run summary. All data comes via props (server-fetched).</done>
</task>

</tasks>

<verification>
- `cd apps/web && npx tsc --noEmit` succeeds
- Server action compiles with proper auth, test case gathering, and runBenchmark delegation
- BenchmarkTab component renders all 6 sections with proper conditional logic
- CostTrendChart renders Recharts AreaChart with UTC date formatting
- No hydration-unsafe date formatting (no toLocaleDateString)
</verification>

<success_criteria>
1. triggerBenchmark server action handles auth, training examples vs ad-hoc inputs, and delegates to runBenchmark
2. BenchmarkTab shows quick stats, staleness warning, trigger button with timer, model comparison table, and cost trend chart
3. CostTrendChart renders cost-over-time AreaChart following established Recharts pattern
4. All code compiles without errors
</success_criteria>

<output>
After completion, create `.planning/phases/61-benchmarking-dashboard/61-02-SUMMARY.md`
</output>
