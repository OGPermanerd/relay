---
phase: 61-benchmarking-dashboard
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - packages/db/src/services/benchmark.ts
  - packages/db/src/services/index.ts
  - apps/web/lib/benchmark-runner.ts
autonomous: true

must_haves:
  truths:
    - "Benchmark runs can be created, queried, and updated in the database"
    - "Benchmark results can be inserted and queried per run"
    - "Skill outputs can be generated across multiple Claude models and quality-scored by an AI judge"
  artifacts:
    - path: "packages/db/src/services/benchmark.ts"
      provides: "CRUD for benchmark_runs and benchmark_results"
      exports: ["createBenchmarkRun", "completeBenchmarkRun", "insertBenchmarkResult", "getLatestBenchmarkRun", "getBenchmarkResultsByRun", "getModelComparisonStats", "getCostTrendData", "getTrainingExamples"]
    - path: "apps/web/lib/benchmark-runner.ts"
      provides: "Benchmark execution engine: model calls + AI judge scoring"
      exports: ["runBenchmark"]
  key_links:
    - from: "packages/db/src/services/benchmark.ts"
      to: "packages/db/src/schema/benchmark-runs.ts"
      via: "drizzle insert/select"
      pattern: "benchmarkRuns|benchmarkResults"
    - from: "apps/web/lib/benchmark-runner.ts"
      to: "packages/db/src/services/benchmark.ts"
      via: "createBenchmarkRun, insertBenchmarkResult, completeBenchmarkRun"
      pattern: "import.*from.*@everyskill/db/services"
    - from: "apps/web/lib/benchmark-runner.ts"
      to: "@anthropic-ai/sdk"
      via: "Anthropic client for model execution and quality judging"
      pattern: "new Anthropic"
---

<objective>
Create the benchmark DB service layer and benchmark execution engine.

Purpose: Provide all backend infrastructure for creating, executing, and storing benchmark runs -- from database CRUD to cross-model execution with AI-judged quality scoring.

Output: `packages/db/src/services/benchmark.ts` (DB service) + `apps/web/lib/benchmark-runner.ts` (execution engine) + barrel export updates
</objective>

<execution_context>
@/home/dev/.claude/get-shit-done/workflows/execute-plan.md
</execution_context>

<context>
@packages/db/src/schema/benchmark-runs.ts
@packages/db/src/services/token-measurements.ts
@packages/db/src/services/pricing.ts
@apps/web/lib/ai-review.ts
@packages/db/src/services/index.ts
@packages/db/src/relations/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Benchmark DB service with CRUD and aggregation queries</name>
  <files>packages/db/src/services/benchmark.ts, packages/db/src/services/index.ts</files>
  <action>
Create `packages/db/src/services/benchmark.ts` with these functions:

1. `createBenchmarkRun(params: { tenantId, skillId, triggeredBy, models: string[] })` -- Insert a benchmark_runs row with status="running", startedAt=new Date(), return the row.

2. `completeBenchmarkRun(runId: string, params: { status: "completed"|"failed", bestModel?, bestQualityScore?, cheapestModel?, cheapestCostMicrocents? })` -- Update the run with completedAt=new Date() and the provided summary fields.

3. `insertBenchmarkResult(params: NewBenchmarkResult)` -- Insert a single benchmark_results row, return it.

4. `getLatestBenchmarkRun(skillId: string)` -- Query the most recent benchmark_runs row for this skill using `db.query.benchmarkRuns.findFirst` with `orderBy: desc(benchmarkRuns.createdAt)`, include `{ with: { results: true } }`. Return null if none.

5. `getBenchmarkResultsByRun(runId: string)` -- Query all benchmark_results rows for a run, ordered by modelName then testCaseIndex.

6. `getModelComparisonStats(runId: string)` -- Aggregate benchmark_results by modelName: AVG(qualityScore), AVG(estimatedCostMicrocents), AVG(totalTokens), AVG(latencyMs), COUNT(*) as testCases. Return array of `{ modelName, avgQuality, avgCost, avgTokens, avgLatency, testCases }`. Use raw SQL aggregation with GROUP BY.

7. `getCostTrendData(skillId: string, days: number = 90)` -- Query token_measurements for this skill over the last N days, grouped by date (date_trunc('day')), returning `{ date: string, avgCost: number }[]`. This powers the cost trend chart. Follow the same date truncation pattern from `getSkillFeedbackStats`.

8. `getTrainingExamples(skillId: string)` -- Query skill_feedback rows where feedbackType='training_example' AND status='accepted' for this skill. Return `{ exampleInput, expectedOutput }[]`. If none exist, return empty array.

Import patterns: Use `import { db } from "../client"`, `import { benchmarkRuns, benchmarkResults } from "../schema/benchmark-runs"`, `import { tokenMeasurements } from "../schema/token-measurements"`, `import { skillFeedback } from "../schema/skill-feedback"`, `import { eq, desc, sql, and, gte } from "drizzle-orm"`.

Export types: `BenchmarkRunWithResults`, `ModelComparisonRow`, `CostTrendPoint`.

Then update `packages/db/src/services/index.ts` to export everything from the new benchmark service:
```
export {
  createBenchmarkRun,
  completeBenchmarkRun,
  insertBenchmarkResult,
  getLatestBenchmarkRun,
  getBenchmarkResultsByRun,
  getModelComparisonStats,
  getCostTrendData,
  getTrainingExamples,
  type BenchmarkRunWithResults,
  type ModelComparisonRow,
  type CostTrendPoint,
} from "./benchmark";
```
  </action>
  <verify>Run `cd /home/dev/projects/relay && pnpm build --filter @everyskill/db` to verify the service compiles. Then run `cd /home/dev/projects/relay/apps/web && npx tsc --noEmit` to confirm imports resolve.</verify>
  <done>All 8 benchmark service functions exist and compile. Barrel exports in services/index.ts include all benchmark functions and types.</done>
</task>

<task type="auto">
  <name>Task 2: Benchmark execution engine with cross-model calls and AI quality judging</name>
  <files>apps/web/lib/benchmark-runner.ts</files>
  <action>
Create `apps/web/lib/benchmark-runner.ts` -- the execution engine that runs a skill's test cases across models and judges output quality.

Follow the Anthropic SDK pattern from `apps/web/lib/ai-review.ts` (getClient(), structured output with JSON schema).

**Main export: `runBenchmark(params)`**

```typescript
interface RunBenchmarkParams {
  tenantId: string;
  skillId: string;
  triggeredBy: string;
  skillContent: string;
  skillName: string;
  testCases: { input: string; expectedOutput: string | null }[];
  models?: string[]; // defaults to BENCHMARK_MODELS
}

interface RunBenchmarkResult {
  runId: string;
  status: "completed" | "failed";
  resultCount: number;
  error?: string;
}
```

**Default models constant:**
```typescript
const BENCHMARK_MODELS = [
  "claude-sonnet-4-5-20250929",
  "claude-haiku-4-5-20251001",
];
```
Only Anthropic models (per BENCH-06 -- Anthropic SDK required, OpenAI/Google optional). Two models keeps execution under 60s.

**Execution flow:**
1. Call `createBenchmarkRun()` with the models list
2. For each test case (sequential):
   a. For each model (parallel via `Promise.allSettled()`):
      - Call Anthropic SDK: `client.messages.create({ model, max_tokens: 2048, messages: [{ role: "user", content: buildPrompt(skillContent, testCase.input) }] })`
      - Extract text response, input_tokens, output_tokens from response.usage
      - Calculate latencyMs (Date.now() before/after)
      - Calculate estimatedCostMicrocents via `estimateCostMicrocents(model, inputTokens, outputTokens)`
      - On error: store result with errorMessage, null scores
   b. After all models complete for this test case, judge quality (see below)
3. After all test cases: compute summary (bestModel = highest avg qualityScore, cheapestModel = lowest avg cost)
4. Call `completeBenchmarkRun()` with summary
5. Return `{ runId, status: "completed", resultCount }`

**buildPrompt helper:**
```typescript
function buildPrompt(skillContent: string, testInput: string): string {
  return `You are using the following skill/prompt:\n\n<skill>\n${skillContent}\n</skill>\n\nApply this skill to the following input:\n\n<input>\n${testInput}\n</input>`;
}
```

**Quality judging (BENCH-07):**
Create `judgeQuality(output: string, expectedOutput: string | null, testInput: string)` function.

- Use a SINGLE judge model (Claude Sonnet) to evaluate ALL outputs -- the judge does NOT know which model produced the output (anti-bias: blinded evaluation per BENCH-07).
- Judge prompt system message: "You are an impartial quality evaluator. Score the output on a 0-100 scale. Do not factor in writing style preferences. Focus on correctness, completeness, and usefulness relative to the input and expected output."
- Use JSON schema output: `{ qualityScore: number (0-100), qualityNotes: string, matchesExpected: boolean }`
- If expectedOutput is null, judge only on general quality (instruct: "No expected output is provided. Judge based on helpfulness, correctness, and completeness alone.")
- The judge model should be the constant: `const JUDGE_MODEL = "claude-sonnet-4-5-20250929";`

After judging, call `insertBenchmarkResult()` for each model+testCase combination with all fields populated.

**Error handling:**
- Wrap the entire function in try/catch. On uncaught error, call `completeBenchmarkRun(runId, { status: "failed" })` and return error.
- Individual model call failures (from Promise.allSettled rejected) should NOT fail the entire run -- store the error and continue.

Import `estimateCostMicrocents` from `@everyskill/db/services/pricing`. Import benchmark service functions from `@everyskill/db/services`.
  </action>
  <verify>Run `cd /home/dev/projects/relay/apps/web && npx tsc --noEmit` to confirm the file compiles with all imports resolving.</verify>
  <done>benchmark-runner.ts exports `runBenchmark` function. It creates a DB run, executes skill across 2 default Claude models in parallel per test case, uses blinded AI-judge scoring, computes summary stats, and handles individual model failures gracefully.</done>
</task>

</tasks>

<verification>
- `pnpm build --filter @everyskill/db` succeeds
- `cd apps/web && npx tsc --noEmit` succeeds
- All 8 benchmark service functions exported from barrel
- benchmark-runner.ts compiles and imports resolve
</verification>

<success_criteria>
1. Benchmark DB service has CRUD + aggregation functions for runs, results, model comparison, and cost trend data
2. Benchmark runner can execute a skill across multiple Claude models with parallel calls and AI-judged quality scoring
3. All code compiles without errors
</success_criteria>

<output>
After completion, create `.planning/phases/61-benchmarking-dashboard/61-01-SUMMARY.md`
</output>
